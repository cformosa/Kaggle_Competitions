{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\290002494\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import modules\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq # Used to read the data\n",
    "import os \n",
    "import numpy as np\n",
    "from keras.layers import * # Keras is the most friendly Neural Network library, this Kernel use a lot of layers classes\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm # Processing time measurement\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras import backend as K # The backend give us access to tensorflow operations and allow us to create the Attention class\n",
    "from keras import optimizers # Allow us to access the Adam class to modify some parameters\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold # Used to use Kfold to train our model\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this for my attention\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is what the competition is judged on\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>signal_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id_measurement</th>\n",
       "      <th>phase</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      signal_id  target\n",
       "id_measurement phase                   \n",
       "0              0              0       0\n",
       "               1              1       0\n",
       "               2              2       0\n",
       "1              0              3       1\n",
       "               1              4       1\n",
       "               2              5       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just load train data\n",
    "df_train = pd.read_csv('metadata_train.csv')\n",
    "# set index, it makes the data access much faster\n",
    "df_train = df_train.set_index(['id_measurement', 'phase'])\n",
    "df_train.head(6)\n",
    "\n",
    "#signal_id 0,1,2 are the 3 that all relate to id_measurement 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800000, 8712)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the parquet files\n",
    "train = pq.read_pandas('train.parquet').to_pandas()\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>8702</th>\n",
       "      <th>8703</th>\n",
       "      <th>8704</th>\n",
       "      <th>8705</th>\n",
       "      <th>8706</th>\n",
       "      <th>8707</th>\n",
       "      <th>8708</th>\n",
       "      <th>8709</th>\n",
       "      <th>8710</th>\n",
       "      <th>8711</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>-19</td>\n",
       "      <td>-16</td>\n",
       "      <td>-5</td>\n",
       "      <td>19</td>\n",
       "      <td>-15</td>\n",
       "      <td>15</td>\n",
       "      <td>-1</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>-22</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>-21</td>\n",
       "      <td>-15</td>\n",
       "      <td>-9</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>-19</td>\n",
       "      <td>-17</td>\n",
       "      <td>-6</td>\n",
       "      <td>19</td>\n",
       "      <td>-17</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>-15</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>-21</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>-19</td>\n",
       "      <td>-15</td>\n",
       "      <td>-8</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>-1</td>\n",
       "      <td>-20</td>\n",
       "      <td>-17</td>\n",
       "      <td>-6</td>\n",
       "      <td>19</td>\n",
       "      <td>-17</td>\n",
       "      <td>15</td>\n",
       "      <td>-3</td>\n",
       "      <td>-15</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>-21</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>-18</td>\n",
       "      <td>-14</td>\n",
       "      <td>-8</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>-19</td>\n",
       "      <td>-16</td>\n",
       "      <td>-5</td>\n",
       "      <td>20</td>\n",
       "      <td>-16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>-15</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>-21</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>-19</td>\n",
       "      <td>-14</td>\n",
       "      <td>-7</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>-19</td>\n",
       "      <td>-16</td>\n",
       "      <td>-5</td>\n",
       "      <td>20</td>\n",
       "      <td>-17</td>\n",
       "      <td>16</td>\n",
       "      <td>-2</td>\n",
       "      <td>-14</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>-22</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>-18</td>\n",
       "      <td>-14</td>\n",
       "      <td>-8</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8712 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1   2   3  4   5   6   7  8   9  ...   8702  8703  8704  8705  8706  \\\n",
       "0  18  1 -19 -16 -5  19 -15  15 -1 -16  ...     18   -22    12     8    13   \n",
       "1  18  0 -19 -17 -6  19 -17  16  0 -15  ...     17   -21    12     8    14   \n",
       "2  17 -1 -20 -17 -6  19 -17  15 -3 -15  ...     16   -21    13     8    15   \n",
       "3  18  1 -19 -16 -5  20 -16  16  0 -15  ...     16   -21    12     8    15   \n",
       "4  18  0 -19 -16 -5  20 -17  16 -2 -14  ...     17   -22    12     8    15   \n",
       "\n",
       "   8707  8708  8709  8710  8711  \n",
       "0     6   -21   -15    -9    20  \n",
       "1     7   -19   -15    -8    21  \n",
       "2     8   -18   -14    -8    22  \n",
       "3     8   -19   -14    -7    23  \n",
       "4     8   -18   -14    -8    23  \n",
       "\n",
       "[5 rows x 8712 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural networks train faster on standardized data\n",
    "max_num = 127\n",
    "min_num = -128\n",
    "def min_max_transf(ts, min_data, max_data, range_needed=(-1,1)):\n",
    "    if min_data < 0:\n",
    "        ts_std = (ts + abs(min_data)) / (max_data + abs(min_data))\n",
    "    else:\n",
    "        ts_std = (ts - min_data) / (max_data - min_data)\n",
    "    if range_needed[0] < 0:    \n",
    "        return ts_std * (range_needed[1] + abs(range_needed[0])) + range_needed[0]\n",
    "    else:\n",
    "        return ts_std * (range_needed[1] - range_needed[0]) + range_needed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is one of the most important peace of code of this Kernel\n",
    "# Any power line contain 3 phases of 800000 measurements, or 2.4 millions data \n",
    "# It would be praticaly impossible to build a NN with an input of that size\n",
    "# The ideia here is to reduce it each phase to a matrix of <n_dim> bins by n features\n",
    "# Each bean is a set of 5000 measurements (800000 / 160), so the features are extracted from this 5000 chunk data.\n",
    "sample_size = 800000\n",
    "def transform_ts(ts, n_dim=160, min_max=(-1,1)):\n",
    "    # convert data into -1 to 1\n",
    "    ts_std = min_max_transf(ts, min_data=min_num, max_data=max_num)\n",
    "    # bucket or chunk size, 5000 in this case (800000 / 160)\n",
    "    bucket_size = int(sample_size / n_dim)\n",
    "    # new_ts will be the container of the new data\n",
    "    new_ts = []\n",
    "    # this for iteract any chunk/bucket until reach the whole sample_size (800000)\n",
    "    for i in range(0, sample_size, bucket_size):\n",
    "        # cut each bucket to ts_range\n",
    "        ts_range = ts_std[i:i + bucket_size]\n",
    "        # calculate each feature\n",
    "        mean = ts_range.mean()\n",
    "        std = ts_range.std() # standard deviation\n",
    "        std_top = mean + std # I have to test it more, but is is like a band\n",
    "        std_bot = mean - std\n",
    "        # I think that the percentiles are very important, it is like a distribuiton analysis from eath chunk\n",
    "        percentil_calc = np.percentile(ts_range, [0, 1, 25, 50, 75, 99, 100]) \n",
    "        max_range = percentil_calc[-1] - percentil_calc[0] # this is the amplitude of the chunk\n",
    "        relative_percentile = percentil_calc - mean # maybe it could heap to understand the asymmetry\n",
    "        # now, we just add all the features to new_ts and convert it to np.array\n",
    "        new_ts.append(np.concatenate([np.asarray([mean, std, std_top, std_bot, max_range]),percentil_calc, relative_percentile]))\n",
    "    return np.asarray(new_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function take a piece of data and convert using transform_ts(), but it does to each of the 3 phases\n",
    "# if we would try to do in one time, could exceed the RAM Memmory\n",
    "def prep_data(start, end):\n",
    "    # load a piece of data from file\n",
    "    X = []\n",
    "    y = []\n",
    "    # using tdqm to evaluate processing time\n",
    "    # takes each index from df_train and iteract it from start to end\n",
    "    # it is divided by 3 because for each id_measurement there are 3 id_signal, and the start/end parameters are id_signal\n",
    "    for id_measurement in tqdm(df_train.index.levels[0].unique()[0:df_train.index.max()[0]]): #0 to 2903\n",
    "        X_signal = []\n",
    "        # for each phase of the signal\n",
    "        for phase in [0,1,2]:\n",
    "            # extract from df_train both signal_id and target to compose the new data sets\n",
    "            signal_id, target = df_train.loc[id_measurement].loc[phase]\n",
    "            # but just append the target one time, to not triplicate it\n",
    "            if phase == 0:\n",
    "                y.append(target)\n",
    "            #for each signal id (not measurement id) do feature engineering\n",
    "            X_signal.append(transform_ts(train[str(signal_id)]))\n",
    "        # concatenate all the 3 phases so they are all in one row\n",
    "        X_signal = np.concatenate(X_signal, axis=1)\n",
    "        # add the data to X\n",
    "        X.append(X_signal)\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2903/2903 [12:40<00:00,  3.93it/s]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "def load_all():\n",
    "    total_size = len(df_train)\n",
    "    for ini, end in [(0, total_size)]:\n",
    "        X_temp, y_temp = prep_data(ini, end)\n",
    "        X.append(X_temp)\n",
    "        y.append(y_temp)\n",
    "load_all()\n",
    "X = np.concatenate(X)\n",
    "y = np.concatenate(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2903, 160, 57) (2903,)\n"
     ]
    }
   ],
   "source": [
    "#x[0] is the number of id_measurements\n",
    "#x[1] is the number of chunks of data\n",
    "#x[2] is the number of features for each\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14322353,  0.00713529,  0.15035882,  0.13608824,  0.04705882,\n",
       "        0.12156863,  0.12941176,  0.1372549 ,  0.14509804,  0.14509804,\n",
       "        0.16078431,  0.16862745, -0.0216549 , -0.01381176, -0.00596863,\n",
       "        0.00187451,  0.00187451,  0.01756078,  0.02540392,  0.0085051 ,\n",
       "        0.00690299,  0.01540809,  0.00160211,  0.05490196, -0.01960784,\n",
       "       -0.00392157,  0.00392157,  0.01176471,  0.01176471,  0.01960784,\n",
       "        0.03529412, -0.02811294, -0.01242667, -0.00458353,  0.00325961,\n",
       "        0.00325961,  0.01110275,  0.02678902, -0.15020235,  0.00815639,\n",
       "       -0.14204596, -0.15835875,  0.07058824, -0.18431373, -0.16862745,\n",
       "       -0.15294118, -0.15294118, -0.14509804, -0.12941176, -0.11372549,\n",
       "       -0.03411137, -0.0184251 , -0.00273882, -0.00273882,  0.00510431,\n",
       "        0.02079059,  0.03647686])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "def model_lstm(input_shape):\n",
    "    inp = Input(shape=(input_shape[1], input_shape[2],))\n",
    "    \n",
    "    #bidirectional works the best when we are dealing with Attention\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(inp) #CuDNNLSTM is faster but needs a gpu\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    \n",
    "    #ATTENTION GOES HERE\n",
    "    x = Attention(input_shape[1])(x)\n",
    "    \n",
    "    #fully connect on the way out to help deal with nonlinear outputs\n",
    "    x = Dense(64, activation = \"relu\")(x)\n",
    "    \n",
    "    #binary classification\n",
    "    x = Dense(1, activation = \"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    \n",
    "    #compile our model\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics=[matthews_correlation])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 160, 57)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 160, 256)          190464    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 160, 128)          164352    \n",
      "_________________________________________________________________\n",
      "attention_1 (Attention)      (None, 128)               288       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 363,425\n",
      "Trainable params: 363,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_lstm(X.shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning fold 1\n",
      "Train on 2322 samples, validate on 581 samples\n",
      "Epoch 1/5\n",
      "2322/2322 [==============================] - ETA: 1:15 - loss: 0.6924 - matthews_correlation: -0.080 - ETA: 55s - loss: 0.6794 - matthews_correlation: -0.040 - ETA: 47s - loss: 0.6634 - matthews_correlation: -0.02 - ETA: 42s - loss: 0.6540 - matthews_correlation: -0.02 - ETA: 38s - loss: 0.6304 - matthews_correlation: -0.01 - ETA: 34s - loss: 0.6101 - matthews_correlation: -0.01 - ETA: 31s - loss: 0.5794 - matthews_correlation: -0.01 - ETA: 28s - loss: 0.5402 - matthews_correlation: -0.01 - ETA: 26s - loss: 0.5084 - matthews_correlation: -0.00 - ETA: 23s - loss: 0.4783 - matthews_correlation: -0.00 - ETA: 20s - loss: 0.4556 - matthews_correlation: -0.00 - ETA: 17s - loss: 0.4390 - matthews_correlation: -0.00 - ETA: 14s - loss: 0.4248 - matthews_correlation: -0.00 - ETA: 11s - loss: 0.4190 - matthews_correlation: -0.00 - ETA: 8s - loss: 0.4004 - matthews_correlation: -0.0053 - ETA: 6s - loss: 0.3985 - matthews_correlation: -0.005 - ETA: 3s - loss: 0.3818 - matthews_correlation: -0.004 - ETA: 0s - loss: 0.3712 - matthews_correlation: -0.004 - 55s 24ms/step - loss: 0.3711 - matthews_correlation: -0.0044 - val_loss: 0.2344 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.00000, saving model to weights_0.h5\n",
      "Epoch 2/5\n",
      "2322/2322 [==============================] - ETA: 50s - loss: 0.2562 - matthews_correlation: 0.0000e+0 - ETA: 46s - loss: 0.2377 - matthews_correlation: 0.0000e+0 - ETA: 43s - loss: 0.2399 - matthews_correlation: 0.0000e+0 - ETA: 40s - loss: 0.2375 - matthews_correlation: 0.0000e+0 - ETA: 38s - loss: 0.2362 - matthews_correlation: 0.0000e+0 - ETA: 34s - loss: 0.2337 - matthews_correlation: 0.0000e+0 - ETA: 32s - loss: 0.2350 - matthews_correlation: 0.0000e+0 - ETA: 29s - loss: 0.2305 - matthews_correlation: 0.0000e+0 - ETA: 26s - loss: 0.2356 - matthews_correlation: 0.0000e+0 - ETA: 23s - loss: 0.2269 - matthews_correlation: 0.0000e+0 - ETA: 20s - loss: 0.2297 - matthews_correlation: 0.0000e+0 - ETA: 17s - loss: 0.2281 - matthews_correlation: 0.0000e+0 - ETA: 14s - loss: 0.2344 - matthews_correlation: 0.0000e+0 - ETA: 11s - loss: 0.2310 - matthews_correlation: 0.0000e+0 - ETA: 9s - loss: 0.2314 - matthews_correlation: 0.0000e+0 - ETA: 6s - loss: 0.2301 - matthews_correlation: 0.0000e+ - ETA: 3s - loss: 0.2304 - matthews_correlation: 0.0000e+ - ETA: 0s - loss: 0.2330 - matthews_correlation: 0.0000e+ - 55s 24ms/step - loss: 0.2351 - matthews_correlation: 0.0000e+00 - val_loss: 0.2290 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 3/5\n",
      "2322/2322 [==============================] - ETA: 49s - loss: 0.1472 - matthews_correlation: 0.0000e+0 - ETA: 47s - loss: 0.1806 - matthews_correlation: 0.0000e+0 - ETA: 44s - loss: 0.1771 - matthews_correlation: 0.0000e+0 - ETA: 41s - loss: 0.1900 - matthews_correlation: 0.0000e+0 - ETA: 37s - loss: 0.1980 - matthews_correlation: 0.0000e+0 - ETA: 34s - loss: 0.2062 - matthews_correlation: 0.0000e+0 - ETA: 32s - loss: 0.2142 - matthews_correlation: 0.0000e+0 - ETA: 28s - loss: 0.2146 - matthews_correlation: 0.0000e+0 - ETA: 25s - loss: 0.2179 - matthews_correlation: 0.0000e+0 - ETA: 23s - loss: 0.2244 - matthews_correlation: 0.0000e+0 - ETA: 20s - loss: 0.2262 - matthews_correlation: 0.0000e+0 - ETA: 17s - loss: 0.2250 - matthews_correlation: 0.0000e+0 - ETA: 14s - loss: 0.2237 - matthews_correlation: 0.0000e+0 - ETA: 11s - loss: 0.2265 - matthews_correlation: 0.0000e+0 - ETA: 8s - loss: 0.2271 - matthews_correlation: 0.0000e+0 - ETA: 5s - loss: 0.2283 - matthews_correlation: 0.0000e+ - ETA: 3s - loss: 0.2258 - matthews_correlation: 0.0000e+ - ETA: 0s - loss: 0.2208 - matthews_correlation: 0.0000e+ - 54s 23ms/step - loss: 0.2201 - matthews_correlation: 0.0000e+00 - val_loss: 0.2164 - val_matthews_correlation: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_matthews_correlation did not improve from 0.00000\n",
      "Epoch 4/5\n",
      "2322/2322 [==============================] - ETA: 56s - loss: 0.3057 - matthews_correlation: 0.0000e+0 - ETA: 49s - loss: 0.2103 - matthews_correlation: 0.0000e+0 - ETA: 45s - loss: 0.2194 - matthews_correlation: 0.0000e+0 - ETA: 42s - loss: 0.2263 - matthews_correlation: 0.0000e+0 - ETA: 38s - loss: 0.2116 - matthews_correlation: 0.0000e+0 - ETA: 35s - loss: 0.2199 - matthews_correlation: 0.0000e+0 - ETA: 32s - loss: 0.2136 - matthews_correlation: 0.0000e+0 - ETA: 29s - loss: 0.2051 - matthews_correlation: -0.0028  - ETA: 26s - loss: 0.2108 - matthews_correlation: -0.00 - ETA: 23s - loss: 0.2092 - matthews_correlation: -0.00 - ETA: 20s - loss: 0.2077 - matthews_correlation: -0.00 - ETA: 17s - loss: 0.2115 - matthews_correlation: -0.00 - ETA: 15s - loss: 0.2044 - matthews_correlation: -0.00 - ETA: 12s - loss: 0.2013 - matthews_correlation: -0.00 - ETA: 9s - loss: 0.2028 - matthews_correlation: -0.0015 - ETA: 6s - loss: 0.2007 - matthews_correlation: -0.001 - ETA: 3s - loss: 0.2027 - matthews_correlation: -0.001 - ETA: 0s - loss: 0.2031 - matthews_correlation: -0.001 - 58s 25ms/step - loss: 0.2030 - matthews_correlation: -0.0012 - val_loss: 0.1971 - val_matthews_correlation: 0.1878\n",
      "\n",
      "Epoch 00004: val_matthews_correlation improved from 0.00000 to 0.18779, saving model to weights_0.h5\n",
      "Epoch 5/5\n",
      "2322/2322 [==============================] - ETA: 53s - loss: 0.1845 - matthews_correlation: 0.271 - ETA: 51s - loss: 0.1616 - matthews_correlation: 0.285 - ETA: 48s - loss: 0.1719 - matthews_correlation: 0.190 - ETA: 46s - loss: 0.1845 - matthews_correlation: 0.142 - ETA: 42s - loss: 0.1915 - matthews_correlation: 0.114 - ETA: 38s - loss: 0.1903 - matthews_correlation: 0.095 - ETA: 34s - loss: 0.1887 - matthews_correlation: 0.081 - ETA: 30s - loss: 0.1913 - matthews_correlation: 0.111 - ETA: 27s - loss: 0.1887 - matthews_correlation: 0.145 - ETA: 25s - loss: 0.1857 - matthews_correlation: 0.168 - ETA: 22s - loss: 0.1862 - matthews_correlation: 0.194 - ETA: 18s - loss: 0.1868 - matthews_correlation: 0.198 - ETA: 15s - loss: 0.1859 - matthews_correlation: 0.198 - ETA: 12s - loss: 0.1848 - matthews_correlation: 0.220 - ETA: 9s - loss: 0.1870 - matthews_correlation: 0.220 - ETA: 6s - loss: 0.1921 - matthews_correlation: 0.21 - ETA: 3s - loss: 0.1920 - matthews_correlation: 0.20 - ETA: 0s - loss: 0.1939 - matthews_correlation: 0.20 - 58s 25ms/step - loss: 0.1932 - matthews_correlation: 0.2059 - val_loss: 0.1662 - val_matthews_correlation: 0.1871\n",
      "\n",
      "Epoch 00005: val_matthews_correlation did not improve from 0.18779\n",
      "Beginning fold 2\n"
     ]
    }
   ],
   "source": [
    "#use this if you want to 5 fold cross validate\n",
    "# First, create a set of indexes of the 5 folds. Shuffle is good in DL so you don't do correlated learning\n",
    "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=2019).split(X, y))\n",
    "preds_val = []\n",
    "y_val = []\n",
    "# Then, iteract with each fold\n",
    "# If you dont know, enumerate(['a', 'b', 'c']) returns [(0, 'a'), (1, 'b'), (2, 'c')]\n",
    "for idx, (train_idx, val_idx) in enumerate(splits):\n",
    "    K.clear_session() # I dont know what it do, but I imagine that it \"clear session\" :)\n",
    "    print(\"Beginning fold {}\".format(idx+1))\n",
    "    # use the indexes to extract the folds in the train and validation data\n",
    "    train_X, train_y, val_X, val_y = X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n",
    "    # instantiate the model for this fold\n",
    "    model = model_lstm(train_X.shape)\n",
    "    # This checkpoint helps to avoid overfitting. It just save the weights of the model if it delivered an\n",
    "    # validation matthews_correlation greater than the last one.\n",
    "    ckpt = ModelCheckpoint('weights_{}.h5'.format(idx), save_best_only=True, save_weights_only=True, verbose=1, monitor='val_matthews_correlation', mode='max')\n",
    "    # Train, train, train\n",
    "    model.fit(train_X, train_y, batch_size=128, epochs=5, validation_data=[val_X, val_y], callbacks=[ckpt])\n",
    "    # loads the best weights saved by the checkpoint\n",
    "    model.load_weights('weights_{}.h5'.format(idx))\n",
    "    # Add the predictions of the validation to the list preds_val\n",
    "    preds_val.append(model.predict(val_X, batch_size=512))\n",
    "    # and the val true y\n",
    "    y_val.append(val_y)\n",
    "\n",
    "# concatenates all and prints the shape    \n",
    "preds_val = np.concatenate(preds_val)[...,0]\n",
    "y_val = np.concatenate(y_val)\n",
    "print(preds_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2903/2903 [==============================] - ETA: 55s - loss: 0.3100 - matthews_correlation: 0.0000e+0 - ETA: 53s - loss: 0.2404 - matthews_correlation: 0.0000e+0 - ETA: 51s - loss: 0.2307 - matthews_correlation: 0.0000e+0 - ETA: 48s - loss: 0.2159 - matthews_correlation: 0.0000e+0 - ETA: 46s - loss: 0.2195 - matthews_correlation: 0.0000e+0 - ETA: 43s - loss: 0.2061 - matthews_correlation: 0.0000e+0 - ETA: 40s - loss: 0.2000 - matthews_correlation: 0.0000e+0 - ETA: 38s - loss: 0.2151 - matthews_correlation: 0.0000e+0 - ETA: 35s - loss: 0.2152 - matthews_correlation: 0.0000e+0 - ETA: 32s - loss: 0.2023 - matthews_correlation: 0.0000e+0 - ETA: 30s - loss: 0.2034 - matthews_correlation: 0.0000e+0 - ETA: 27s - loss: 0.2132 - matthews_correlation: 0.0000e+0 - ETA: 25s - loss: 0.2114 - matthews_correlation: 0.0000e+0 - ETA: 22s - loss: 0.2148 - matthews_correlation: 0.0000e+0 - ETA: 20s - loss: 0.2127 - matthews_correlation: 0.0000e+0 - ETA: 17s - loss: 0.2167 - matthews_correlation: 0.0000e+0 - ETA: 14s - loss: 0.2125 - matthews_correlation: 0.0000e+0 - ETA: 12s - loss: 0.2148 - matthews_correlation: 0.0000e+0 - ETA: 9s - loss: 0.2203 - matthews_correlation: 0.0000e+0 - ETA: 7s - loss: 0.2222 - matthews_correlation: 0.0000e+ - ETA: 4s - loss: 0.2296 - matthews_correlation: 0.0000e+ - ETA: 1s - loss: 0.2291 - matthews_correlation: 0.0000e+ - 60s 21ms/step - loss: 0.2296 - matthews_correlation: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2903/2903 [==============================] - ETA: 55s - loss: 0.1719 - matthews_correlation: 0.0000e+0 - ETA: 55s - loss: 0.1652 - matthews_correlation: 0.0000e+0 - ETA: 51s - loss: 0.1743 - matthews_correlation: 0.0000e+0 - ETA: 48s - loss: 0.1914 - matthews_correlation: 0.0000e+0 - ETA: 46s - loss: 0.2099 - matthews_correlation: 0.0000e+0 - ETA: 44s - loss: 0.1979 - matthews_correlation: 0.0000e+0 - ETA: 41s - loss: 0.2027 - matthews_correlation: 0.0000e+0 - ETA: 38s - loss: 0.2007 - matthews_correlation: 0.0000e+0 - ETA: 35s - loss: 0.1993 - matthews_correlation: 0.0000e+0 - ETA: 33s - loss: 0.2074 - matthews_correlation: 0.0000e+0 - ETA: 30s - loss: 0.2209 - matthews_correlation: 0.0000e+0 - ETA: 28s - loss: 0.2203 - matthews_correlation: 0.0000e+0 - ETA: 25s - loss: 0.2210 - matthews_correlation: 0.0000e+0 - ETA: 22s - loss: 0.2248 - matthews_correlation: 0.0000e+0 - ETA: 20s - loss: 0.2253 - matthews_correlation: 0.0000e+0 - ETA: 17s - loss: 0.2218 - matthews_correlation: 0.0000e+0 - ETA: 14s - loss: 0.2222 - matthews_correlation: 0.0000e+0 - ETA: 12s - loss: 0.2224 - matthews_correlation: 0.0000e+0 - ETA: 9s - loss: 0.2238 - matthews_correlation: 0.0000e+0 - ETA: 7s - loss: 0.2250 - matthews_correlation: 0.0000e+ - ETA: 4s - loss: 0.2271 - matthews_correlation: 0.0000e+ - ETA: 1s - loss: 0.2283 - matthews_correlation: 0.0000e+ - 62s 21ms/step - loss: 0.2281 - matthews_correlation: 0.0000e+00\n",
      "Epoch 3/10\n",
      "2903/2903 [==============================] - ETA: 1:00 - loss: 0.2041 - matthews_correlation: 0.0000e+ - ETA: 58s - loss: 0.1738 - matthews_correlation: 0.0000e+00 - ETA: 56s - loss: 0.1743 - matthews_correlation: 0.0000e+0 - ETA: 52s - loss: 0.1777 - matthews_correlation: 0.0000e+0 - ETA: 48s - loss: 0.2078 - matthews_correlation: 0.0000e+0 - ETA: 46s - loss: 0.2008 - matthews_correlation: 0.0000e+0 - ETA: 42s - loss: 0.2062 - matthews_correlation: 0.0000e+0 - ETA: 40s - loss: 0.2058 - matthews_correlation: 0.0000e+0 - ETA: 37s - loss: 0.2065 - matthews_correlation: 0.0000e+0 - ETA: 34s - loss: 0.2091 - matthews_correlation: 0.0000e+0 - ETA: 31s - loss: 0.2273 - matthews_correlation: 0.0000e+0 - ETA: 28s - loss: 0.2266 - matthews_correlation: 0.0000e+0 - ETA: 26s - loss: 0.2248 - matthews_correlation: 0.0000e+0 - ETA: 23s - loss: 0.2228 - matthews_correlation: 0.0000e+0 - ETA: 20s - loss: 0.2242 - matthews_correlation: 0.0000e+0 - ETA: 17s - loss: 0.2232 - matthews_correlation: 0.0000e+0 - ETA: 15s - loss: 0.2192 - matthews_correlation: 0.0000e+0 - ETA: 12s - loss: 0.2210 - matthews_correlation: 0.0000e+0 - ETA: 9s - loss: 0.2209 - matthews_correlation: 0.0000e+0 - ETA: 7s - loss: 0.2194 - matthews_correlation: 0.0000e+ - ETA: 4s - loss: 0.2238 - matthews_correlation: 0.0000e+ - ETA: 1s - loss: 0.2197 - matthews_correlation: 0.0000e+ - 61s 21ms/step - loss: 0.2176 - matthews_correlation: 0.0000e+00\n",
      "Epoch 4/10\n",
      "2903/2903 [==============================] - ETA: 55s - loss: 0.1418 - matthews_correlation: 0.0000e+0 - ETA: 54s - loss: 0.1522 - matthews_correlation: 0.0000e+0 - ETA: 52s - loss: 0.1572 - matthews_correlation: 0.0000e+0 - ETA: 49s - loss: 0.1803 - matthews_correlation: 0.0000e+0 - ETA: 46s - loss: 0.1835 - matthews_correlation: 0.0000e+0 - ETA: 45s - loss: 0.1880 - matthews_correlation: 0.0000e+0 - ETA: 44s - loss: 0.1818 - matthews_correlation: 0.0279    - ETA: 42s - loss: 0.1771 - matthews_correlation: 0.024 - ETA: 40s - loss: 0.1697 - matthews_correlation: 0.021 - ETA: 38s - loss: 0.1797 - matthews_correlation: 0.019 - ETA: 35s - loss: 0.1829 - matthews_correlation: 0.017 - ETA: 32s - loss: 0.1862 - matthews_correlation: 0.016 - ETA: 29s - loss: 0.1919 - matthews_correlation: 0.012 - ETA: 26s - loss: 0.1869 - matthews_correlation: 0.010 - ETA: 22s - loss: 0.1871 - matthews_correlation: 0.009 - ETA: 19s - loss: 0.1860 - matthews_correlation: 0.007 - ETA: 16s - loss: 0.1920 - matthews_correlation: 0.012 - ETA: 13s - loss: 0.1911 - matthews_correlation: 0.019 - ETA: 10s - loss: 0.1919 - matthews_correlation: 0.035 - ETA: 7s - loss: 0.2008 - matthews_correlation: 0.034 - ETA: 4s - loss: 0.2008 - matthews_correlation: 0.04 - ETA: 1s - loss: 0.2037 - matthews_correlation: 0.04 - 66s 23ms/step - loss: 0.2036 - matthews_correlation: 0.0573\n",
      "Epoch 5/10\n",
      "2903/2903 [==============================] - ETA: 59s - loss: 0.2024 - matthews_correlation: 0.0000e+0 - ETA: 55s - loss: 0.2120 - matthews_correlation: 0.0000e+0 - ETA: 52s - loss: 0.2377 - matthews_correlation: 0.0000e+0 - ETA: 50s - loss: 0.2142 - matthews_correlation: 0.0000e+0 - ETA: 47s - loss: 0.2119 - matthews_correlation: 0.0000e+0 - ETA: 45s - loss: 0.1868 - matthews_correlation: 0.0000e+0 - ETA: 43s - loss: 0.1935 - matthews_correlation: 0.0000e+0 - ETA: 41s - loss: 0.1908 - matthews_correlation: 0.0000e+0 - ETA: 38s - loss: 0.1871 - matthews_correlation: 0.0000e+0 - ETA: 35s - loss: 0.1913 - matthews_correlation: 0.0000e+0 - ETA: 32s - loss: 0.1870 - matthews_correlation: 0.0000e+0 - ETA: 29s - loss: 0.1860 - matthews_correlation: 0.0000e+0 - ETA: 26s - loss: 0.1832 - matthews_correlation: 0.0000e+0 - ETA: 23s - loss: 0.1803 - matthews_correlation: 0.0000e+0 - ETA: 20s - loss: 0.1933 - matthews_correlation: 0.0000e+0 - ETA: 18s - loss: 0.1932 - matthews_correlation: 0.0000e+0 - ETA: 15s - loss: 0.1937 - matthews_correlation: 0.0000e+0 - ETA: 12s - loss: 0.1915 - matthews_correlation: 0.0000e+0 - ETA: 9s - loss: 0.1895 - matthews_correlation: 0.0000e+0 - ETA: 7s - loss: 0.1914 - matthews_correlation: 0.0000e+ - ETA: 4s - loss: 0.1901 - matthews_correlation: 0.0000e+ - ETA: 1s - loss: 0.1867 - matthews_correlation: 0.0000e+ - 62s 21ms/step - loss: 0.1833 - matthews_correlation: 0.0000e+00\n",
      "Epoch 6/10\n",
      "2903/2903 [==============================] - ETA: 57s - loss: 0.1758 - matthews_correlation: 0.0000e+0 - ETA: 54s - loss: 0.2055 - matthews_correlation: 0.0000e+0 - ETA: 51s - loss: 0.1956 - matthews_correlation: 0.0000e+0 - ETA: 48s - loss: 0.1829 - matthews_correlation: 0.0000e+0 - ETA: 46s - loss: 0.1757 - matthews_correlation: 0.0000e+0 - ETA: 43s - loss: 0.1746 - matthews_correlation: 0.0000e+0 - ETA: 41s - loss: 0.1812 - matthews_correlation: 0.0000e+0 - ETA: 38s - loss: 0.1822 - matthews_correlation: 0.0000e+0 - ETA: 35s - loss: 0.1883 - matthews_correlation: 0.0000e+0 - ETA: 33s - loss: 0.1870 - matthews_correlation: 0.0000e+0 - ETA: 30s - loss: 0.1863 - matthews_correlation: 0.0000e+0 - ETA: 28s - loss: 0.1812 - matthews_correlation: 0.0000e+0 - ETA: 25s - loss: 0.1749 - matthews_correlation: 0.0000e+0 - ETA: 22s - loss: 0.1777 - matthews_correlation: 0.0000e+0 - ETA: 20s - loss: 0.1786 - matthews_correlation: 0.0000e+0 - ETA: 17s - loss: 0.1778 - matthews_correlation: 0.0000e+0 - ETA: 15s - loss: 0.1746 - matthews_correlation: 0.0000e+0 - ETA: 12s - loss: 0.1757 - matthews_correlation: 0.0000e+0 - ETA: 9s - loss: 0.1794 - matthews_correlation: 0.0000e+0 - ETA: 7s - loss: 0.1795 - matthews_correlation: 0.0000e+ - ETA: 4s - loss: 0.1754 - matthews_correlation: 0.0000e+ - ETA: 1s - loss: 0.1721 - matthews_correlation: 0.0000e+ - 62s 21ms/step - loss: 0.1689 - matthews_correlation: 0.0000e+00\n",
      "Epoch 7/10\n",
      "2903/2903 [==============================] - ETA: 1:00 - loss: 0.2666 - matthews_correlation: 0.0000e+ - ETA: 58s - loss: 0.2628 - matthews_correlation: 0.0000e+00 - ETA: 54s - loss: 0.2050 - matthews_correlation: 0.0000e+0 - ETA: 51s - loss: 0.1715 - matthews_correlation: 0.0000e+0 - ETA: 47s - loss: 0.1627 - matthews_correlation: 0.0000e+0 - ETA: 44s - loss: 0.1630 - matthews_correlation: 0.0000e+0 - ETA: 41s - loss: 0.1573 - matthews_correlation: 0.0000e+0 - ETA: 39s - loss: 0.1529 - matthews_correlation: 0.0000e+0 - ETA: 37s - loss: 0.1484 - matthews_correlation: -0.0022  - ETA: 34s - loss: 0.1515 - matthews_correlation: -0.00 - ETA: 31s - loss: 0.1450 - matthews_correlation: -0.00 - ETA: 28s - loss: 0.1512 - matthews_correlation: -0.00 - ETA: 25s - loss: 0.1506 - matthews_correlation: -0.00 - ETA: 23s - loss: 0.1543 - matthews_correlation: 0.0082 - ETA: 20s - loss: 0.1524 - matthews_correlation: 0.066 - ETA: 18s - loss: 0.1504 - matthews_correlation: 0.106 - ETA: 15s - loss: 0.1494 - matthews_correlation: 0.140 - ETA: 12s - loss: 0.1489 - matthews_correlation: 0.169 - ETA: 9s - loss: 0.1475 - matthews_correlation: 0.192 - ETA: 7s - loss: 0.1480 - matthews_correlation: 0.21 - ETA: 4s - loss: 0.1470 - matthews_correlation: 0.24 - ETA: 1s - loss: 0.1460 - matthews_correlation: 0.26 - 62s 21ms/step - loss: 0.1442 - matthews_correlation: 0.2771\n",
      "Epoch 8/10\n",
      "2903/2903 [==============================] - ETA: 1:01 - loss: 0.2506 - matthews_correlation: 0.48 - ETA: 56s - loss: 0.1868 - matthews_correlation: 0.5902 - ETA: 53s - loss: 0.1413 - matthews_correlation: 0.499 - ETA: 51s - loss: 0.1336 - matthews_correlation: 0.524 - ETA: 48s - loss: 0.1307 - matthews_correlation: 0.536 - ETA: 45s - loss: 0.1372 - matthews_correlation: 0.531 - ETA: 42s - loss: 0.1379 - matthews_correlation: 0.518 - ETA: 40s - loss: 0.1322 - matthews_correlation: 0.554 - ETA: 37s - loss: 0.1303 - matthews_correlation: 0.551 - ETA: 34s - loss: 0.1294 - matthews_correlation: 0.567 - ETA: 31s - loss: 0.1247 - matthews_correlation: 0.585 - ETA: 28s - loss: 0.1242 - matthews_correlation: 0.596 - ETA: 26s - loss: 0.1256 - matthews_correlation: 0.600 - ETA: 23s - loss: 0.1232 - matthews_correlation: 0.613 - ETA: 20s - loss: 0.1208 - matthews_correlation: 0.612 - ETA: 17s - loss: 0.1222 - matthews_correlation: 0.593 - ETA: 15s - loss: 0.1201 - matthews_correlation: 0.601 - ETA: 12s - loss: 0.1171 - matthews_correlation: 0.595 - ETA: 9s - loss: 0.1147 - matthews_correlation: 0.600 - ETA: 7s - loss: 0.1178 - matthews_correlation: 0.60 - ETA: 4s - loss: 0.1190 - matthews_correlation: 0.60 - ETA: 1s - loss: 0.1174 - matthews_correlation: 0.61 - 62s 21ms/step - loss: 0.1164 - matthews_correlation: 0.6153\n",
      "Epoch 9/10\n",
      "2903/2903 [==============================] - ETA: 1:05 - loss: 0.0902 - matthews_correlation: 0.73 - ETA: 59s - loss: 0.0974 - matthews_correlation: 0.7333 - ETA: 55s - loss: 0.1031 - matthews_correlation: 0.709 - ETA: 51s - loss: 0.1071 - matthews_correlation: 0.632 - ETA: 48s - loss: 0.1269 - matthews_correlation: 0.633 - ETA: 45s - loss: 0.1418 - matthews_correlation: 0.593 - ETA: 43s - loss: 0.1373 - matthews_correlation: 0.616 - ETA: 40s - loss: 0.1332 - matthews_correlation: 0.611 - ETA: 37s - loss: 0.1267 - matthews_correlation: 0.633 - ETA: 34s - loss: 0.1220 - matthews_correlation: 0.634 - ETA: 31s - loss: 0.1213 - matthews_correlation: 0.637 - ETA: 29s - loss: 0.1238 - matthews_correlation: 0.619 - ETA: 26s - loss: 0.1292 - matthews_correlation: 0.615 - ETA: 23s - loss: 0.1290 - matthews_correlation: 0.602 - ETA: 21s - loss: 0.1304 - matthews_correlation: 0.586 - ETA: 18s - loss: 0.1309 - matthews_correlation: 0.592 - ETA: 15s - loss: 0.1292 - matthews_correlation: 0.601 - ETA: 12s - loss: 0.1275 - matthews_correlation: 0.603 - ETA: 10s - loss: 0.1243 - matthews_correlation: 0.611 - ETA: 7s - loss: 0.1202 - matthews_correlation: 0.622 - ETA: 4s - loss: 0.1187 - matthews_correlation: 0.61 - ETA: 1s - loss: 0.1199 - matthews_correlation: 0.62 - 65s 22ms/step - loss: 0.1179 - matthews_correlation: 0.6208\n",
      "Epoch 10/10\n",
      "2903/2903 [==============================] - ETA: 1:26 - loss: 0.1455 - matthews_correlation: 0.51 - ETA: 1:24 - loss: 0.1430 - matthews_correlation: 0.60 - ETA: 1:17 - loss: 0.1294 - matthews_correlation: 0.63 - ETA: 1:12 - loss: 0.1248 - matthews_correlation: 0.60 - ETA: 1:07 - loss: 0.1289 - matthews_correlation: 0.62 - ETA: 1:02 - loss: 0.1229 - matthews_correlation: 0.63 - ETA: 56s - loss: 0.1190 - matthews_correlation: 0.6534 - ETA: 51s - loss: 0.1166 - matthews_correlation: 0.648 - ETA: 46s - loss: 0.1176 - matthews_correlation: 0.630 - ETA: 42s - loss: 0.1144 - matthews_correlation: 0.644 - ETA: 39s - loss: 0.1083 - matthews_correlation: 0.669 - ETA: 36s - loss: 0.1057 - matthews_correlation: 0.665 - ETA: 33s - loss: 0.1062 - matthews_correlation: 0.653 - ETA: 29s - loss: 0.1103 - matthews_correlation: 0.651 - ETA: 25s - loss: 0.1077 - matthews_correlation: 0.665 - ETA: 22s - loss: 0.1099 - matthews_correlation: 0.667 - ETA: 19s - loss: 0.1090 - matthews_correlation: 0.673 - ETA: 15s - loss: 0.1107 - matthews_correlation: 0.668 - ETA: 12s - loss: 0.1093 - matthews_correlation: 0.669 - ETA: 8s - loss: 0.1053 - matthews_correlation: 0.676 - ETA: 5s - loss: 0.1059 - matthews_correlation: 0.67 - ETA: 2s - loss: 0.1074 - matthews_correlation: 0.67 - 74s 25ms/step - loss: 0.1079 - matthews_correlation: 0.6763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cc101d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use this if you want to avoid cross validation\n",
    "model.fit(X,y, epochs=10, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output of this kernel must be binary (0 or 1), but the output of the NN Model is float (0 to 1).\n",
    "# So, find the best threshold to convert float to binary is crucial to the result\n",
    "# this piece of code is a function that evaluates all the possible thresholds from 0 to 1 by 0.01\n",
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.05 for i in range(100)]):\n",
    "        score = K.eval(matthews_correlation(y_true.astype(np.float64), (y_proba > threshold).astype(np.float64)))\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'matthews_correlation': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = threshold_search(y_val, preds_val)['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8709, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "for preds in pred:\n",
    "    for i in range(3):\n",
    "        predictions.append(preds)\n",
    "np.array(predictions).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2903,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.round(predictions,0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
